{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa88861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2742101a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2bad87280e40ad82002dcb08c2f91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:490\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    489\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2838\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2829\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2831\u001b[0m     (\n\u001b[1;32m   2832\u001b[0m         model,\n\u001b[1;32m   2833\u001b[0m         missing_keys,\n\u001b[1;32m   2834\u001b[0m         unexpected_keys,\n\u001b[1;32m   2835\u001b[0m         mismatched_keys,\n\u001b[1;32m   2836\u001b[0m         offload_index,\n\u001b[1;32m   2837\u001b[0m         error_msgs,\n\u001b[0;32m-> 2838\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2856\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   2857\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3184\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3174\u001b[0m mismatched_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3175\u001b[0m     state_dict,\n\u001b[1;32m   3176\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3180\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3181\u001b[0m )\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[0;32m-> 3184\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3191\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3192\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3200\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:724\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    721\u001b[0m             fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m param_name:\n\u001b[0;32m--> 724\u001b[0m             \u001b[43mset_module_quantized_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/bitsandbytes.py:101\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m     99\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 101\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348c0ae8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_model_for_kbit_training\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=None, \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08e4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c7c342a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584ced74ad3b44908beeeaed233a72a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/Abirate--english_quotes to /root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef91c689c400452e892d154030e5047c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e177667e37044778a6b48a00a7655e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/647k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9a75834d26483085c926a2ca957dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b6762ffbe74edfa4e00c881b8d6747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"Abirate/english_quotes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e2a2fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m [quote \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This quote was made by: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m author \u001b[38;5;28;01mfor\u001b[39;00m quote, author \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquote\u001b[39m\u001b[38;5;124m'\u001b[39m], samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(text)\n\u001b[0;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mmap(tokenize_data, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize_data(samples):\n",
    "    text = [quote + \" --> \" + author for quote, author in zip(samples['quote'], samples['author'])]\n",
    "    return tokenizer(text)\n",
    "\n",
    "data = data.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7725fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quote': '“So many books, so little time.”',\n",
       " 'author': 'Frank Zappa',\n",
       " 'tags': ['books', 'humor'],\n",
       " 'input_ids': [1628,\n",
       "  2598,\n",
       "  1142,\n",
       "  5098,\n",
       "  13,\n",
       "  594,\n",
       "  1652,\n",
       "  673,\n",
       "  1425,\n",
       "  831,\n",
       "  14430,\n",
       "  369,\n",
       "  1160,\n",
       "  407,\n",
       "  27,\n",
       "  6893,\n",
       "  1503,\n",
       "  5596],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a093971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-5d0a116e382eebe3.arrow\n"
     ]
    }
   ],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bf1fee19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> “Be yourself; everyone else is already taken.”'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_tokenized['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c08c2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quote': '“Be yourself; everyone else is already taken.”',\n",
       " 'author': 'Oscar Wilde',\n",
       " 'tags': ['be-yourself',\n",
       "  'gilbert-perreira',\n",
       "  'honesty',\n",
       "  'inspirational',\n",
       "  'misattributed-oscar-wilde',\n",
       "  'quote-investigator']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d405b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/Abirate___json/Abirate--french_book_reviews-d8564dff8c279fae/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864780687e94437ab22ee4e1ffbb01dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data2 = load_dataset(\"Abirate/french_book_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86d8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09fdc58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de67f7d0fe464638a8643863fa5ea45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'quote'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquote\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:851\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 851\u001b[0m     {\n\u001b[1;32m    852\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:852\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    851\u001b[0m     {\n\u001b[0;32m--> 852\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3066\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3067\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3068\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3074\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3075\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3449\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3445\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3446\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3447\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3449\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3453\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3458\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3330\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3329\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3330\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3332\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3333\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3334\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(samples)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m samples: tokenizer(\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquote\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m), batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:270\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 270\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    272\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'quote'"
     ]
    }
   ],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3925f587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34589393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca50b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acronym_identification',\n",
       " 'ade_corpus_v2',\n",
       " 'adversarial_qa',\n",
       " 'aeslc',\n",
       " 'afrikaans_ner_corpus',\n",
       " 'ag_news',\n",
       " 'ai2_arc',\n",
       " 'air_dialogue',\n",
       " 'ajgt_twitter_ar',\n",
       " 'allegro_reviews',\n",
       " 'allocine',\n",
       " 'alt',\n",
       " 'amazon_polarity',\n",
       " 'amazon_reviews_multi',\n",
       " 'amazon_us_reviews',\n",
       " 'ambig_qa',\n",
       " 'americas_nli',\n",
       " 'ami',\n",
       " 'amttl',\n",
       " 'anli',\n",
       " 'app_reviews',\n",
       " 'aqua_rat',\n",
       " 'aquamuse',\n",
       " 'ar_cov19',\n",
       " 'ar_res_reviews',\n",
       " 'ar_sarcasm',\n",
       " 'arabic_billion_words',\n",
       " 'arabic_pos_dialect',\n",
       " 'arabic_speech_corpus',\n",
       " 'arcd',\n",
       " 'arsentd_lev',\n",
       " 'art',\n",
       " 'arxiv_dataset',\n",
       " 'ascent_kb',\n",
       " 'aslg_pc12',\n",
       " 'asnq',\n",
       " 'asset',\n",
       " 'assin',\n",
       " 'assin2',\n",
       " 'atomic',\n",
       " 'autshumato',\n",
       " 'facebook/babi_qa',\n",
       " 'banking77',\n",
       " 'bbaw_egyptian',\n",
       " 'bbc_hindi_nli',\n",
       " 'bc2gm_corpus',\n",
       " 'beans',\n",
       " 'best2009',\n",
       " 'bianet',\n",
       " 'bible_para',\n",
       " 'big_patent',\n",
       " 'billsum',\n",
       " 'bing_coronavirus_query_set',\n",
       " 'biomrc',\n",
       " 'biosses',\n",
       " 'blbooks',\n",
       " 'blbooksgenre',\n",
       " 'blended_skill_talk',\n",
       " 'blimp',\n",
       " 'blog_authorship_corpus',\n",
       " 'bn_hate_speech',\n",
       " 'bnl_newspapers',\n",
       " 'bookcorpus',\n",
       " 'bookcorpusopen',\n",
       " 'boolq',\n",
       " 'bprec',\n",
       " 'break_data',\n",
       " 'brwac',\n",
       " 'bsd_ja_en',\n",
       " 'bswac',\n",
       " 'c3',\n",
       " 'c4',\n",
       " 'cail2018',\n",
       " 'caner',\n",
       " 'capes',\n",
       " 'casino',\n",
       " 'catalonia_independence',\n",
       " 'cats_vs_dogs',\n",
       " 'cawac',\n",
       " 'cbt',\n",
       " 'cc100',\n",
       " 'cc_news',\n",
       " 'ccaligned_multilingual',\n",
       " 'cdsc',\n",
       " 'cdt',\n",
       " 'cedr',\n",
       " 'cfq',\n",
       " 'chr_en',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'circa',\n",
       " 'civil_comments',\n",
       " 'clickbait_news_bg',\n",
       " 'climate_fever',\n",
       " 'clinc_oos',\n",
       " 'clue',\n",
       " 'cmrc2018',\n",
       " 'cmu_hinglish_dog',\n",
       " 'cnn_dailymail',\n",
       " 'coached_conv_pref',\n",
       " 'coarse_discourse',\n",
       " 'codah',\n",
       " 'code_search_net',\n",
       " 'code_x_glue_cc_clone_detection_big_clone_bench',\n",
       " 'code_x_glue_cc_clone_detection_poj104',\n",
       " 'code_x_glue_cc_cloze_testing_all',\n",
       " 'code_x_glue_cc_cloze_testing_maxmin',\n",
       " 'code_x_glue_cc_code_completion_line',\n",
       " 'code_x_glue_cc_code_completion_token',\n",
       " 'code_x_glue_cc_code_refinement',\n",
       " 'code_x_glue_cc_code_to_code_trans',\n",
       " 'code_x_glue_cc_defect_detection',\n",
       " 'code_x_glue_ct_code_to_text',\n",
       " 'code_x_glue_tc_nl_code_search_adv',\n",
       " 'code_x_glue_tc_text_to_code',\n",
       " 'code_x_glue_tt_text_to_text',\n",
       " 'com_qa',\n",
       " 'common_gen',\n",
       " 'common_language',\n",
       " 'common_voice',\n",
       " 'commonsense_qa',\n",
       " 'competition_math',\n",
       " 'compguesswhat',\n",
       " 'conceptnet5',\n",
       " 'conll2000',\n",
       " 'conll2002',\n",
       " 'conll2003',\n",
       " 'conllpp',\n",
       " 'consumer-finance-complaints',\n",
       " 'conv_ai',\n",
       " 'conv_ai_2',\n",
       " 'conv_ai_3',\n",
       " 'conv_questions',\n",
       " 'coqa',\n",
       " 'allenai/cord19',\n",
       " 'cornell_movie_dialog',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'counter',\n",
       " 'covid_qa_castorini',\n",
       " 'covid_qa_deepset',\n",
       " 'covid_qa_ucsd',\n",
       " 'covid_tweets_japanese',\n",
       " 'covost2',\n",
       " 'cppe-5',\n",
       " 'craigslist_bargains',\n",
       " 'crawl_domain',\n",
       " 'crd3',\n",
       " 'crime_and_punish',\n",
       " 'crows_pairs',\n",
       " 'cryptonite',\n",
       " 'cs_restaurants',\n",
       " 'cuad',\n",
       " 'curiosity_dialogs',\n",
       " 'daily_dialog',\n",
       " 'dane',\n",
       " 'danish_political_comments',\n",
       " 'dart',\n",
       " 'datacommons_factcheck',\n",
       " 'dbpedia_14',\n",
       " 'dbrd',\n",
       " 'deal_or_no_dialog',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dengue_filipino',\n",
       " 'dialog_re',\n",
       " 'diplomacy_detection',\n",
       " 'disaster_response_messages',\n",
       " 'discofuse',\n",
       " 'discovery',\n",
       " 'disfl_qa',\n",
       " 'doc2dial',\n",
       " 'docred',\n",
       " 'doqa',\n",
       " 'dream',\n",
       " 'drop',\n",
       " 'duorc',\n",
       " 'dutch_social',\n",
       " 'dyk',\n",
       " 'e2e_nlg',\n",
       " 'e2e_nlg_cleaned',\n",
       " 'ecb',\n",
       " 'ecthr_cases',\n",
       " 'eduge',\n",
       " 'ehealth_kd',\n",
       " 'eitb_parcc',\n",
       " 'electricity_load_diagrams',\n",
       " 'eli5',\n",
       " 'eli5_category',\n",
       " 'emea',\n",
       " 'emo',\n",
       " 'dair-ai/emotion',\n",
       " 'emotone_ar',\n",
       " 'empathetic_dialogues',\n",
       " 'enriched_web_nlg',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eth_py150_open',\n",
       " 'ethos',\n",
       " 'eu_regulatory_ir',\n",
       " 'eurlex',\n",
       " 'euronews',\n",
       " 'europa_eac_tm',\n",
       " 'europa_ecdc_tm',\n",
       " 'europarl_bilingual',\n",
       " 'event2Mind',\n",
       " 'evidence_infer_treatment',\n",
       " 'exams',\n",
       " 'factckbr',\n",
       " 'fake_news_english',\n",
       " 'fake_news_filipino',\n",
       " 'farsi_news',\n",
       " 'fashion_mnist',\n",
       " 'fever',\n",
       " 'few_rel',\n",
       " 'financial_phrasebank',\n",
       " 'finer',\n",
       " 'flores',\n",
       " 'flue',\n",
       " 'food101',\n",
       " 'fquad',\n",
       " 'freebase_qa',\n",
       " 'gap',\n",
       " 'gem',\n",
       " 'generated_reviews_enth',\n",
       " 'generics_kb',\n",
       " 'german_legal_entity_recognition',\n",
       " 'germaner',\n",
       " 'germeval_14',\n",
       " 'giga_fren',\n",
       " 'gigaword',\n",
       " 'glucose',\n",
       " 'glue',\n",
       " 'gnad10',\n",
       " 'go_emotions',\n",
       " 'gooaq',\n",
       " 'google_wellformed_query',\n",
       " 'grail_qa',\n",
       " 'great_code',\n",
       " 'greek_legal_code',\n",
       " 'guardian_authorship',\n",
       " 'gutenberg_time',\n",
       " 'hans',\n",
       " 'hansards',\n",
       " 'hard',\n",
       " 'harem',\n",
       " 'has_part',\n",
       " 'hate_offensive',\n",
       " 'hate_speech18',\n",
       " 'hate_speech_filipino',\n",
       " 'hate_speech_offensive',\n",
       " 'hate_speech_pl',\n",
       " 'hate_speech_portuguese',\n",
       " 'hatexplain',\n",
       " 'hausa_voa_ner',\n",
       " 'hausa_voa_topics',\n",
       " 'hda_nli_hindi',\n",
       " 'head_qa',\n",
       " 'health_fact',\n",
       " 'hebrew_projectbenyehuda',\n",
       " 'hebrew_sentiment',\n",
       " 'hebrew_this_world',\n",
       " 'hellaswag',\n",
       " 'cais/mmlu',\n",
       " 'hind_encorp',\n",
       " 'hindi_discourse',\n",
       " 'hippocorpus',\n",
       " 'hkcancor',\n",
       " 'hlgd',\n",
       " 'hope_edi',\n",
       " 'hotpot_qa',\n",
       " 'hover',\n",
       " 'hrenwac_para',\n",
       " 'hrwac',\n",
       " 'humicroedit',\n",
       " 'hybrid_qa',\n",
       " 'hyperpartisan_news_detection',\n",
       " 'iapp_wiki_qa_squad',\n",
       " 'id_clickbait',\n",
       " 'id_liputan6',\n",
       " 'id_nergrit_corpus',\n",
       " 'id_newspapers_2018',\n",
       " 'id_panl_bppt',\n",
       " 'id_puisi',\n",
       " 'igbo_english_machine_translation',\n",
       " 'igbo_monolingual',\n",
       " 'igbo_ner',\n",
       " 'ilist',\n",
       " 'imdb',\n",
       " 'imdb_urdu_reviews',\n",
       " 'imppres',\n",
       " 'indic_glue',\n",
       " 'indonli',\n",
       " 'indonlp/indonlu',\n",
       " 'inquisitive_qg',\n",
       " 'interpress_news_category_tr',\n",
       " 'interpress_news_category_tr_lite',\n",
       " 'irc_disentangle',\n",
       " 'isixhosa_ner_corpus',\n",
       " 'isizulu_ner_corpus',\n",
       " 'iwslt2017',\n",
       " 'jeopardy',\n",
       " 'jfleg',\n",
       " 'jigsaw_toxicity_pred',\n",
       " 'jigsaw_unintended_bias',\n",
       " 'jnlpba',\n",
       " 'journalists_questions',\n",
       " 'kan_hope',\n",
       " 'kannada_news',\n",
       " 'kd_conv',\n",
       " 'kde4',\n",
       " 'kelm',\n",
       " 'kilt_tasks',\n",
       " 'kilt_wikipedia',\n",
       " 'kinnews_kirnews',\n",
       " 'klue',\n",
       " 'kor_3i4k',\n",
       " 'kor_hate',\n",
       " 'kor_ner',\n",
       " 'kor_nli',\n",
       " 'kor_nlu',\n",
       " 'kor_qpair',\n",
       " 'kor_sae',\n",
       " 'kor_sarcasm',\n",
       " 'labr',\n",
       " 'lama',\n",
       " 'lambada',\n",
       " 'large_spanish_corpus',\n",
       " 'laroseda',\n",
       " 'lc_quad',\n",
       " 'lener_br',\n",
       " 'lex_glue',\n",
       " 'liar',\n",
       " 'librispeech_asr',\n",
       " 'librispeech_lm',\n",
       " 'limit',\n",
       " 'lince',\n",
       " 'linnaeus',\n",
       " 'liveqa',\n",
       " 'lj_speech',\n",
       " 'lm1b',\n",
       " 'lst20',\n",
       " 'm_lama',\n",
       " 'mac_morpho',\n",
       " 'makhzan',\n",
       " 'masakhaner',\n",
       " 'math_dataset',\n",
       " 'math_qa',\n",
       " 'matinf',\n",
       " 'mbpp',\n",
       " 'mc4',\n",
       " 'mc_taco',\n",
       " 'md_gender_bias',\n",
       " 'mdd',\n",
       " 'med_hop',\n",
       " 'medal',\n",
       " 'medical_dialog',\n",
       " 'medical_questions_pairs',\n",
       " 'menyo20k_mt',\n",
       " 'meta_woz',\n",
       " 'metooma',\n",
       " 'metrec',\n",
       " 'miam',\n",
       " 'mkb',\n",
       " 'mkqa',\n",
       " 'mlqa',\n",
       " 'mlsum',\n",
       " 'mnist',\n",
       " 'mocha',\n",
       " 'moroco',\n",
       " 'movie_rationales',\n",
       " 'mrqa',\n",
       " 'ms_marco',\n",
       " 'ms_terms',\n",
       " 'msr_genomics_kbcomp',\n",
       " 'msr_sqa',\n",
       " 'msr_text_compression',\n",
       " 'msr_zhen_translation_parity',\n",
       " 'msra_ner',\n",
       " 'mt_eng_vietnamese',\n",
       " 'muchocine',\n",
       " 'multi_booked',\n",
       " 'multi_eurlex',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'multi_para_crawl',\n",
       " 'multi_re_qa',\n",
       " 'multi_woz_v22',\n",
       " 'multi_x_science_sum',\n",
       " 'multidoc2dial',\n",
       " 'multilingual_librispeech',\n",
       " 'mutual_friends',\n",
       " 'mwsc',\n",
       " 'myanmar_news',\n",
       " 'narrativeqa',\n",
       " 'narrativeqa_manual',\n",
       " 'natural_questions',\n",
       " 'ncbi_disease',\n",
       " 'nchlt',\n",
       " 'ncslgr',\n",
       " 'nell',\n",
       " 'neural_code_search',\n",
       " 'news_commentary',\n",
       " 'newsgroup',\n",
       " 'newsph',\n",
       " 'newsph_nli',\n",
       " 'newspop',\n",
       " 'newsqa',\n",
       " 'newsroom',\n",
       " 'nkjp-ner',\n",
       " 'nli_tr',\n",
       " 'nlu_evaluation_data',\n",
       " 'norec',\n",
       " 'norne',\n",
       " 'norwegian_ner',\n",
       " 'nq_open',\n",
       " 'nsmc',\n",
       " 'numer_sense',\n",
       " 'numeric_fused_head',\n",
       " 'oclar',\n",
       " 'offcombr',\n",
       " 'offenseval2020_tr',\n",
       " 'offenseval_dravidian',\n",
       " 'ofis_publik',\n",
       " 'ohsumed',\n",
       " 'ollie',\n",
       " 'omp',\n",
       " 'onestop_english',\n",
       " 'onestop_qa',\n",
       " 'open_subtitles',\n",
       " 'openai_humaneval',\n",
       " 'openbookqa',\n",
       " 'openslr',\n",
       " 'openwebtext',\n",
       " 'opinosis',\n",
       " 'opus100',\n",
       " 'opus_books',\n",
       " 'opus_dgt',\n",
       " 'opus_dogc',\n",
       " 'opus_elhuyar',\n",
       " 'opus_euconst',\n",
       " 'opus_finlex',\n",
       " 'opus_fiskmo',\n",
       " 'opus_gnome',\n",
       " 'opus_infopankki',\n",
       " 'opus_memat',\n",
       " 'opus_montenegrinsubs',\n",
       " 'opus_openoffice',\n",
       " 'opus_paracrawl',\n",
       " 'opus_rf',\n",
       " 'opus_tedtalks',\n",
       " 'opus_ubuntu',\n",
       " 'opus_wikipedia',\n",
       " 'opus_xhosanavy',\n",
       " 'orange_sum',\n",
       " 'oscar',\n",
       " 'para_crawl',\n",
       " 'para_pat',\n",
       " 'parsinlu_reading_comprehension',\n",
       " 'pass',\n",
       " 'paws-x',\n",
       " 'paws',\n",
       " 'pec',\n",
       " 'allenai/peer_read',\n",
       " 'peoples_daily_ner',\n",
       " 'per_sent',\n",
       " 'persian_ner',\n",
       " 'pg19',\n",
       " 'php',\n",
       " 'etalab-ia/piaf',\n",
       " 'pib',\n",
       " 'piqa',\n",
       " 'pn_summary',\n",
       " 'poem_sentiment',\n",
       " 'polemo2',\n",
       " 'poleval2019_cyberbullying',\n",
       " 'poleval2019_mt',\n",
       " 'polsum',\n",
       " 'polyglot_ner',\n",
       " 'prachathai67k',\n",
       " 'pragmeval',\n",
       " 'proto_qa',\n",
       " 'psc',\n",
       " 'ptb_text_only',\n",
       " 'pubmed',\n",
       " 'pubmed_qa',\n",
       " 'py_ast',\n",
       " 'qa4mre',\n",
       " 'qa_srl',\n",
       " 'qa_zre',\n",
       " 'qangaroo',\n",
       " 'qanta',\n",
       " 'qasc',\n",
       " 'allenai/qasper',\n",
       " 'qed',\n",
       " 'qed_amara',\n",
       " 'quac',\n",
       " 'quail',\n",
       " 'quarel',\n",
       " 'quartz',\n",
       " 'quora',\n",
       " 'quoref',\n",
       " 'race',\n",
       " 're_dial',\n",
       " 'reasoning_bg',\n",
       " 'recipe_nlg',\n",
       " 'reclor',\n",
       " 'red_caps',\n",
       " 'reddit',\n",
       " 'reddit_tifu',\n",
       " 'refresd',\n",
       " 'reuters21578',\n",
       " 'riddle_sense',\n",
       " 'ro_sent',\n",
       " 'ro_sts',\n",
       " 'ro_sts_parallel',\n",
       " 'roman_urdu',\n",
       " 'ronec',\n",
       " 'ropes',\n",
       " 'rotten_tomatoes',\n",
       " 'RussianNLP/russian_super_glue',\n",
       " 'allenai/s2orc',\n",
       " 'samsum',\n",
       " 'sanskrit_classic',\n",
       " 'saudinewsnet',\n",
       " 'sberquad',\n",
       " 'scan',\n",
       " 'scb_mt_enth_2020',\n",
       " 'scene_parse_150',\n",
       " 'schema_guided_dstc8',\n",
       " 'allenai/scicite',\n",
       " 'scielo',\n",
       " 'scientific_papers',\n",
       " 'allenai/scifact',\n",
       " 'sciq',\n",
       " 'scitail',\n",
       " 'allenai/scitldr',\n",
       " 'search_qa',\n",
       " 'sede',\n",
       " 'selqa',\n",
       " 'sem_eval_2010_task_8',\n",
       " 'sem_eval_2014_task_1',\n",
       " 'sem_eval_2018_task_1',\n",
       " 'sem_eval_2020_task_11',\n",
       " 'sent_comp',\n",
       " 'senti_lex',\n",
       " 'senti_ws',\n",
       " 'sentiment140',\n",
       " 'sepedi_ner',\n",
       " 'sesotho_ner_corpus',\n",
       " 'setimes',\n",
       " 'setswana_ner_corpus',\n",
       " 'sharc',\n",
       " 'sharc_modified',\n",
       " 'sick',\n",
       " 'silicone',\n",
       " 'simple_questions_v2',\n",
       " 'siswati_ner_corpus',\n",
       " 'smartdata',\n",
       " 'sms_spam',\n",
       " 'snips_built_in_intents',\n",
       " 'snli',\n",
       " 'snow_simplified_japanese_corpus',\n",
       " 'so_stacksample',\n",
       " 'social_bias_frames',\n",
       " 'social_i_qa',\n",
       " 'sofc_materials_articles',\n",
       " 'sogou_news',\n",
       " 'spanish_billion_words',\n",
       " 'spc',\n",
       " 'species_800',\n",
       " 'speech_commands',\n",
       " 'spider',\n",
       " 'squad',\n",
       " 'squad_adversarial',\n",
       " 'squad_es',\n",
       " 'squad_it',\n",
       " 'squad_kor_v1',\n",
       " 'squad_kor_v2',\n",
       " 'squad_v1_pt',\n",
       " 'squad_v2',\n",
       " 'squadshifts',\n",
       " 'srwac',\n",
       " 'sst',\n",
       " 'stereoset',\n",
       " 'story_cloze',\n",
       " 'stsb_mt_sv',\n",
       " 'stsb_multi_mt',\n",
       " 'style_change_detection',\n",
       " 'subjqa',\n",
       " 'super_glue',\n",
       " 'superb',\n",
       " 'svhn',\n",
       " 'swag',\n",
       " 'swahili',\n",
       " 'swahili_news',\n",
       " 'swda',\n",
       " 'swedish_medical_ner',\n",
       " 'swedish_ner_corpus',\n",
       " 'swedish_reviews',\n",
       " 'rcds/swiss_judgment_prediction',\n",
       " 'tab_fact',\n",
       " 'tamilmixsentiment',\n",
       " 'tanzil',\n",
       " 'tapaco',\n",
       " 'tashkeela',\n",
       " 'taskmaster1',\n",
       " 'taskmaster2',\n",
       " 'taskmaster3',\n",
       " 'tatoeba',\n",
       " 'ted_hrlr',\n",
       " 'ted_iwlst2013',\n",
       " 'ted_multi',\n",
       " 'ted_talks_iwslt',\n",
       " 'telugu_books',\n",
       " 'telugu_news',\n",
       " 'tep_en_fa_para',\n",
       " 'text2log',\n",
       " 'thai_toxicity_tweet',\n",
       " 'thainer',\n",
       " 'thaiqa_squad',\n",
       " 'thaisum',\n",
       " 'EleutherAI/pile',\n",
       " 'the_pile_books3',\n",
       " 'the_pile_openwebtext2',\n",
       " 'the_pile_stack_exchange',\n",
       " 'tilde_model',\n",
       " 'time_dial',\n",
       " 'times_of_india_news_headlines',\n",
       " 'timit_asr',\n",
       " 'tiny_shakespeare',\n",
       " 'tlc',\n",
       " 'tmu_gfm_dataset',\n",
       " 'told-br',\n",
       " 'totto',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tsac',\n",
       " 'ttc4900',\n",
       " 'tunizi',\n",
       " 'tuple_ie',\n",
       " 'turk',\n",
       " 'turkic_xwmt',\n",
       " 'turkish_movie_sentiment',\n",
       " 'turkish_ner',\n",
       " 'turkish_product_reviews',\n",
       " 'turkish_shrinked_ner',\n",
       " 'turku_ner_corpus',\n",
       " 'tweet_eval',\n",
       " 'tweet_qa',\n",
       " 'tweets_ar_en_parallel',\n",
       " 'tweets_hate_speech_detection',\n",
       " 'twi_text_c3',\n",
       " 'twi_wordsim353',\n",
       " 'tydiqa',\n",
       " 'ubuntu_dialogs_corpus',\n",
       " 'udhr',\n",
       " 'um005',\n",
       " 'un_ga',\n",
       " 'un_multi',\n",
       " 'un_pc',\n",
       " 'universal_dependencies',\n",
       " 'universal_morphologies',\n",
       " 'urdu_fake_news',\n",
       " 'urdu_sentiment_corpus',\n",
       " 'vctk',\n",
       " 'vivos',\n",
       " 'web_nlg',\n",
       " 'web_of_science',\n",
       " 'web_questions',\n",
       " 'weibo_ner',\n",
       " 'wi_locness',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wiki_asp',\n",
       " 'wiki_atomic_edits',\n",
       " 'wiki_auto',\n",
       " 'wiki_bio',\n",
       " 'wiki_dpr',\n",
       " 'wiki_hop',\n",
       " 'wiki_lingua',\n",
       " 'wiki_movies',\n",
       " 'wiki_qa',\n",
       " 'wiki_qa_ar',\n",
       " 'wiki_snippets',\n",
       " 'wiki_source',\n",
       " 'wiki_split',\n",
       " 'wiki_summary',\n",
       " 'wikiann',\n",
       " 'wikicorpus',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikisql',\n",
       " 'wikitext',\n",
       " 'wikitext_tl39',\n",
       " 'wili_2018',\n",
       " 'wino_bias',\n",
       " 'winograd_wsc',\n",
       " 'winogrande',\n",
       " 'wiqa',\n",
       " 'wisesight1000',\n",
       " 'wisesight_sentiment',\n",
       " 'wmt14',\n",
       " 'wmt15',\n",
       " 'wmt16',\n",
       " 'wmt17',\n",
       " 'wmt18',\n",
       " 'wmt19',\n",
       " 'wmt20_mlqe_task1',\n",
       " 'wmt20_mlqe_task2',\n",
       " 'wmt20_mlqe_task3',\n",
       " 'wmt_t2t',\n",
       " 'wnut_17',\n",
       " 'wongnai_reviews',\n",
       " 'woz_dialogue',\n",
       " 'wrbsc',\n",
       " 'x_stance',\n",
       " 'xcopa',\n",
       " 'xcsr',\n",
       " 'xed_en_fi',\n",
       " 'xglue',\n",
       " 'xnli',\n",
       " 'xor_tydi_qa',\n",
       " 'xquad',\n",
       " 'xquad_r',\n",
       " 'xsum',\n",
       " 'xsum_factuality',\n",
       " 'xtreme',\n",
       " 'yahoo_answers_qa',\n",
       " 'yahoo_answers_topics',\n",
       " 'yelp_polarity',\n",
       " 'yelp_review_full',\n",
       " 'yoruba_bbc_topics',\n",
       " 'yoruba_gv_ner',\n",
       " 'yoruba_text_c3',\n",
       " 'yoruba_wordsim353',\n",
       " 'youtube_caption_corrections',\n",
       " 'zest',\n",
       " '0n1xus/codexglue',\n",
       " '0n1xus/pytorrent-standalone',\n",
       " 'AConsApart/anime_subtitles_DialoGPT',\n",
       " 'AHussain0418/day2_data',\n",
       " 'AHussain0418/day4data',\n",
       " 'AHussain0418/demo_data',\n",
       " 'AI-Sweden/SuperLim',\n",
       " 'AI-it/khs_service_test',\n",
       " 'AI-it/korean-hate-speech',\n",
       " 'ARKseal/YFCC14M_subset_webdataset',\n",
       " 'ARTeLab/fanpage',\n",
       " 'ARTeLab/ilpost',\n",
       " 'ARTeLab/mlsum-it',\n",
       " 'ASCCCCCCCC/amazon_zh',\n",
       " 'ASCCCCCCCC/amazon_zh_simple',\n",
       " 'Abdo1Kamr/Arabic_Hadith',\n",
       " 'Abirate/code_net_dataset',\n",
       " 'Abirate/code_net_dev_dataset',\n",
       " 'Abirate/code_net_test_final_dataset',\n",
       " 'Abirate/english_quotes',\n",
       " 'Abirate/french_book_reviews',\n",
       " 'AdWeeb/DravidianMT',\n",
       " 'Adnan/Urdu_News_Headlines',\n",
       " 'AhmadSawal/qa',\n",
       " 'AhmedSSoliman/CoNaLa',\n",
       " 'Aisha/BAAD16',\n",
       " 'Aisha/BAAD6',\n",
       " 'Akila/ForgottenRealmsWikiDataset',\n",
       " 'Akshith/aa',\n",
       " 'Akshith/g_rock',\n",
       " 'Akshith/test',\n",
       " 'adorkin/extended_tweet_emojis',\n",
       " 'AlekseyKorshuk/comedy-scripts',\n",
       " 'AlekseyKorshuk/horror-scripts',\n",
       " 'AlexMaclean/all-deletion-compressions',\n",
       " 'AlexMaclean/wikipedia-deletion-compressions',\n",
       " 'AlexZapolskii/zapolskii-amazon',\n",
       " 'AlgoveraAI/CryptoPunks',\n",
       " 'Aliseyfi/event_token_type',\n",
       " 'Alvenir/nst-da-16khz',\n",
       " 'AndrewMcDowell/de_corpora_parliament_processed',\n",
       " 'Annabelleabbott/real-fake-news-workshop',\n",
       " 'Annielytics/DoctorsNotes',\n",
       " 'Anurag-Singh-creator/task',\n",
       " 'Anurag-Singh-creator/tasks',\n",
       " 'ApiInferenceTest/asr_dummy',\n",
       " 'Arnold/hausa_common_voice',\n",
       " 'AryanLala/autonlp-data-Scientific_Title_Generator',\n",
       " 'Atsushi/fungi_diagnostic_chars_comparison_japanese',\n",
       " 'Atsushi/fungi_indexed_mycological_papers_japanese',\n",
       " 'Atsushi/fungi_trait_circus_database',\n",
       " 'Avishekavi/Avi',\n",
       " 'BSC-LT/SQAC',\n",
       " 'BSC-LT/ancora-ca-ner',\n",
       " 'BSC-LT/sts-ca',\n",
       " 'BSC-LT/tecla',\n",
       " 'BSC-LT/viquiquad',\n",
       " 'BSC-LT/xquad-ca',\n",
       " 'Babelscape/rebel-dataset',\n",
       " 'Babelscape/wikineural',\n",
       " 'BatuhanYilmaz/github-issues',\n",
       " 'Baybars/parla_text_corpus',\n",
       " 'BeIR/beir-corpus',\n",
       " 'BeIR/beir',\n",
       " 'Lacito/pangloss',\n",
       " 'Binbin/my_dataset',\n",
       " 'BlakesOrb6/Fred-Flintstone',\n",
       " 'Bosio/pacman',\n",
       " 'Bosio/pacman_descriptions',\n",
       " 'BritishLibraryLabs/EThOS-PhD-metadata',\n",
       " 'CAGER/rick',\n",
       " 'CALM/arwiki',\n",
       " 'CAiRE/ASCEND',\n",
       " 'CShorten/KerasBERT',\n",
       " 'ChadxxxxHall/Inter-vision',\n",
       " 'Champion/vpc2020_clear_anon_speech',\n",
       " 'Check/a_re_gi',\n",
       " 'Check/region_1',\n",
       " 'Check/region_2',\n",
       " 'Check/region_3',\n",
       " 'Check/region_4',\n",
       " 'Check/region_5',\n",
       " 'Check/region_6',\n",
       " 'Check/region_7',\n",
       " 'Check/region_8',\n",
       " 'Check/region_9',\n",
       " 'Check/regions',\n",
       " 'Check/vverify',\n",
       " 'Cheranga/test',\n",
       " 'ChristophSchuhmann/MS_COCO_2017_URL_TEXT',\n",
       " 'Chun/dataset',\n",
       " 'Chuu/Vhh',\n",
       " 'CodedotAI/code-clippy-tfrecords',\n",
       " 'CodedotAI/code_clippy',\n",
       " 'CodedotAI/code_clippy_github',\n",
       " 'Crives/haha',\n",
       " 'Cropinky/flatearther',\n",
       " 'Cropinky/rap_lyrics_english',\n",
       " 'Cropinky/wow_fishing_bobber',\n",
       " 'Cyberfish/pos_tagger',\n",
       " 'Cyberfish/text_error_correction',\n",
       " 'CyranoB/polarity',\n",
       " 'DDSC/angry-tweets',\n",
       " 'DDSC/dkhate',\n",
       " 'DDSC/europarl',\n",
       " 'DDSC/lcc',\n",
       " 'DDSC/partial-danish-gigaword-no-twitter',\n",
       " 'DDSC/reddit-da-asr-preprocessed',\n",
       " 'DDSC/reddit-da',\n",
       " 'DDSC/twitter-sent',\n",
       " 'DELith/github-issues',\n",
       " 'DSCI511G1/COP26_Energy_Transition_Tweets',\n",
       " 'DanL/scientific-challenges-and-directions-dataset',\n",
       " 'Daniele/dante-corpus',\n",
       " 'Darren/data',\n",
       " 'Datatang/accented_english',\n",
       " 'Datatang/accented_mandarin',\n",
       " 'Datatang/chinese_dialect',\n",
       " 'Datatang/mandarin_chinese',\n",
       " 'Datatang/mixed_speech_chinese_english',\n",
       " 'Datatang/multi_language',\n",
       " 'Datatang/multi_language_conversation',\n",
       " 'Davlan/conll2003_de_noMISC',\n",
       " 'Davlan/conll2003_noMISC',\n",
       " 'Davlan/masakhanerV1',\n",
       " 'DelgadoPanadero/Pokemon',\n",
       " 'DeskDown/ALTDataset',\n",
       " 'DeskDown/ALTDataset_en-to-fil-vi-id-ms-ja-khm',\n",
       " 'DiFronzo/Human_Activity_Recognition',\n",
       " 'Dmitriy612/1',\n",
       " 'DoctorSlimm/yipee',\n",
       " 'Doohae/klue-mrc-bm25',\n",
       " 'Doohae/modern_music_re',\n",
       " 'DoyyingFace/github-embeddings-doy',\n",
       " 'DoyyingFace/github-issues-doy',\n",
       " 'DrishtiSharma/as_opus100_processed',\n",
       " 'DrishtiSharma/bg_opus100_processed',\n",
       " 'DrishtiSharma/br_opus100_processed',\n",
       " 'DrishtiSharma/hi_opus100_processed',\n",
       " 'DrishtiSharma/kk_opus100_processed',\n",
       " 'DrishtiSharma/mr_opus100_processed',\n",
       " 'DrishtiSharma/or_opus100_processed',\n",
       " 'DrishtiSharma/sl_opus100_processed',\n",
       " 'DrishtiSharma/sr_opus100_processed',\n",
       " 'Dumiiii/common-voice-romaniarss',\n",
       " 'EMBO/biolang',\n",
       " 'EMBO/sd-nlp',\n",
       " 'ESZER/H',\n",
       " 'Emanuel/UD_Portuguese-Bosque',\n",
       " 'Emma121/aaaaa',\n",
       " 'Emma121/testtest',\n",
       " 'Enes3774/data',\n",
       " 'Exr0n/wiki-entity-similarity',\n",
       " 'Eymen3455/xsum_tr',\n",
       " 'FIG-Loneliness/FIG-Loneliness',\n",
       " 'FL33TW00D/test-dataset',\n",
       " 'FRTNX/cosuju',\n",
       " 'FRTNX/worldbank-projects',\n",
       " 'Felix-ML/quoteli3',\n",
       " 'Finnish-NLP/mc4_fi_cleaned',\n",
       " 'Firoj/HumAID',\n",
       " 'Francois/futures_es',\n",
       " 'Fraser/mnist-text-default',\n",
       " 'Fraser/mnist-text-no-spaces',\n",
       " 'Fraser/mnist-text-small',\n",
       " 'Fraser/dream-coder',\n",
       " 'Fraser/python-lines',\n",
       " 'Fraser/python-state-changes',\n",
       " 'Fraser/short-jokes',\n",
       " 'Fraser/wiki_sentences',\n",
       " 'GEM/ART',\n",
       " 'GEM/BiSECT',\n",
       " 'GEM/CrossWOZ',\n",
       " 'GEM/OrangeSum',\n",
       " 'GEM/RiSAWOZ',\n",
       " 'GEM/RotoWire_English-German',\n",
       " 'GEM/SIMPITIKI',\n",
       " 'GEM/SciDuet',\n",
       " 'GEM/Taskmaster',\n",
       " 'GEM/cochrane-simplification',\n",
       " 'GEM/common_gen',\n",
       " 'GEM/conversational_weather',\n",
       " 'GEM/cs_restaurants',\n",
       " 'GEM/dart',\n",
       " 'GEM/dstc10_track2_task2',\n",
       " 'GEM/e2e_nlg',\n",
       " 'GEM/indonlg',\n",
       " 'GEM/mlb_data_to_text',\n",
       " 'GEM/mlsum',\n",
       " 'GEM/opusparcus',\n",
       " 'GEM/references',\n",
       " 'GEM/schema_guided_dialog',\n",
       " 'GEM/sportsett_basketball',\n",
       " 'GEM/squad_v2',\n",
       " 'GEM/surface_realisation_st_2020',\n",
       " 'GEM/totto',\n",
       " 'GEM/turku_hockey_data2text',\n",
       " 'GEM/turku_paraphrase_corpus',\n",
       " 'GEM-submissions/v1-outputs-and-scores',\n",
       " 'GEM/viggo',\n",
       " 'GEM/web_nlg',\n",
       " 'GEM/wiki_auto_asset_turk',\n",
       " 'GEM/wiki_cat_sum',\n",
       " 'GEM/wiki_lingua',\n",
       " 'GEM/xlsum',\n",
       " 'GEM/xsum',\n",
       " 'GEM-submissions/GEM__bart_base_schema_guided_dialog__1645547915',\n",
       " 'GEM-submissions/Leo__bart-large__1645784880',\n",
       " 'GEM-submissions/Leo__mbart-large-cc25__1645802644',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1645558682',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1645559101',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1645800191',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1646049378',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1646049424',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1646049601',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1646049876',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1646050898',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1646051364',\n",
       " 'GEM-submissions/lewtun__hugging-face-test-t5-base.outputs.json-36bf2a59__1646052073',\n",
       " 'GEM-submissions/lewtun__this-is-a-test__1646052811',\n",
       " 'GEM-submissions/lewtun__this-is-a-test__1646230987',\n",
       " 'GEM-submissions/ratishsp',\n",
       " 'GEM-submissions/submission-scores',\n",
       " 'GV05/shlomit_speech',\n",
       " 'Gabriel/quora_swe',\n",
       " 'GalacticAI/Noirset',\n",
       " 'Gauravadlakha1509/new_one',\n",
       " 'GeoffVdr/cv8_trainval_processed',\n",
       " 'GonzaloA/fake_news',\n",
       " 'Graphcore/gqa-lxmert',\n",
       " 'Graphcore/gqa',\n",
       " 'Graphcore/vqa-lxmert',\n",
       " 'Graphcore/vqa',\n",
       " 'Graphcore/wikipedia-bert-128',\n",
       " 'Graphcore/wikipedia-bert-512',\n",
       " 'GroNLP/ik-nlp-22_pestyle',\n",
       " 'GroNLP/ik-nlp-22_slp',\n",
       " 'GroNLP/ik-nlp-22_transqe',\n",
       " 'GroNLP/ik-nlp-22_winemag',\n",
       " 'Gwangho/NCBI-Sars-Cov-2',\n",
       " 'HHousen/ParaSCI',\n",
       " 'HHousen/msrp',\n",
       " 'HHousen/quora',\n",
       " 'HUPD/hupd',\n",
       " 'Halilyesilceng/autonlp-data-nameEntityRecognition',\n",
       " 'HarleyQ/WitcherDialogue',\n",
       " 'HarrisDePerceptron/sv_corpora_parliament_processed',\n",
       " 'HarrisDePerceptron/ur_corpora_pib',\n",
       " 'Harveenchadha/bol-models',\n",
       " 'Harveenchadha/indic-voice',\n",
       " 'HarveyBWest/mybot',\n",
       " 'Hellisotherpeople/DebateSum',\n",
       " 'Helsinki-NLP/tatoeba_mt',\n",
       " 'HenryAI/KerasAPIReference.txt',\n",
       " 'HenryAI/KerasBERTv1-Data',\n",
       " 'HenryAI/KerasCodeExamples.txt',\n",
       " 'HenryAI/KerasDeveloperGuides.txt',\n",
       " 'Huertas97/autonlp-data-mami-semeval-20-21',\n",
       " 'Husain/intent-classification-en-fr',\n",
       " 'IFSTalfredoswald/MBTI',\n",
       " 'Iftoo95/Arabic_Sentiment_and_Topics',\n",
       " 'IlyaGusev/gazeta',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0ac64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b910974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['quote', 'author', 'tags', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2508\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "526a060e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> “I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at times hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at my best.” Marilyn Monroe\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a928d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c928201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ae7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b339c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:230: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.491300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.144000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7, training_loss=2.5385754449026927, metrics={'train_runtime': 38.9258, 'train_samples_per_second': 0.719, 'train_steps_per_second': 0.18, 'total_flos': 77651543900160.0, 'train_loss': 2.5385754449026927, 'epoch': 0.01})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=7,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447de80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37db14af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"“Be yourself; everyone else is already taken.”  This quote was made by: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99ec1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "594922e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2131bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {key: value.to(device) for key, value in inputs.items() if key != 'token_type_ids'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "add83314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1628,  4678,  4834,    28,  4130,  2010,   310,  2168,  2668,  1425,\n",
       "          50276,  1552, 14430,   369,  1160,   407,    27,   209]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "703a3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93d03d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f2e8328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1628, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00cbddec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Be yourself; everyone else is already taken.”  This quote was made by: \\ne.g.:\\n\\nGeorge Bernard Shaw'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "063f2123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Be yourself; everyone else is already taken.” --> --> --> --> --> --> --> --> --> -->'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352b851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d8152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
