{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc75fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077529d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0d895",
   "metadata": {},
   "source": [
    "# Preparing the Datasets\n",
    "\n",
    "The two datasets that were used here were: \n",
    "<p> note that for this dataset, I simply downloaded off huggingface hub since the load_dataset wasn't working immediately\n",
    "<p>\n",
    "https://huggingface.co/datasets/Hello-SimpleAI/HC3/blob/main/all.jsonl \n",
    "\n",
    "\n",
    "https://huggingface.co/datasets/MohamedRashad/ChatGPT-prompts/tree/main\n",
    "\n",
    "This tutorial is going to be heavily influenced by this guide:\n",
    "https://plainenglish.io/blog/creating-your-own-chatgpt-a-guide-to-fine-tuning-llms-with-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c1597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('datasets/Hello-SimpleAI-HC3-all.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625f5e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>human_answers</th>\n",
       "      <th>chatgpt_answers</th>\n",
       "      <th>index</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is every book I hear about a \" NY Times # ...</td>\n",
       "      <td>[Basically there are many categories of \" Best...</td>\n",
       "      <td>[There are many different best seller lists th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If salt is so bad for cars , why do we use it ...</td>\n",
       "      <td>[salt is good for not dying in car crashes and...</td>\n",
       "      <td>[Salt is used on roads to help melt ice and sn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do we still have SD TV channels when HD lo...</td>\n",
       "      <td>[The way it works is that old TV stations got ...</td>\n",
       "      <td>[There are a few reasons why we still have SD ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why has nobody assassinated Kim Jong - un He i...</td>\n",
       "      <td>[You ca n't just go around assassinating the l...</td>\n",
       "      <td>[It is generally not acceptable or ethical to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How was airplane technology able to advance so...</td>\n",
       "      <td>[Wanting to kill the shit out of Germans drive...</td>\n",
       "      <td>[After the Wright Brothers made the first powe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit_eli5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Why is every book I hear about a \" NY Times # ...   \n",
       "1  If salt is so bad for cars , why do we use it ...   \n",
       "2  Why do we still have SD TV channels when HD lo...   \n",
       "3  Why has nobody assassinated Kim Jong - un He i...   \n",
       "4  How was airplane technology able to advance so...   \n",
       "\n",
       "                                       human_answers  \\\n",
       "0  [Basically there are many categories of \" Best...   \n",
       "1  [salt is good for not dying in car crashes and...   \n",
       "2  [The way it works is that old TV stations got ...   \n",
       "3  [You ca n't just go around assassinating the l...   \n",
       "4  [Wanting to kill the shit out of Germans drive...   \n",
       "\n",
       "                                     chatgpt_answers  index       source  \n",
       "0  [There are many different best seller lists th...    NaN  reddit_eli5  \n",
       "1  [Salt is used on roads to help melt ice and sn...    NaN  reddit_eli5  \n",
       "2  [There are a few reasons why we still have SD ...    NaN  reddit_eli5  \n",
       "3  [It is generally not acceptable or ethical to ...    NaN  reddit_eli5  \n",
       "4  [After the Wright Brothers made the first powe...    NaN  reddit_eli5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce45d5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24322"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0533a22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/MohamedRashad___csv/MohamedRashad--ChatGPT-prompts-29296547fda20fc1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6a155196db47249ce9aa59590b4472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a70620923fd4b6cb799736cf954d781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "438165"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "dataset2 = load_dataset('MohamedRashad/ChatGPT-prompts')\n",
    "\n",
    "# save it to the directory that I want it at\n",
    "dataset2['train'].to_json(path_or_buf='datasets/MohamedRashad-ChatGPT-prompts.jsonl',lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a19ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human_prompt': 'Write a detailed product description for a food chopper tool that lets you chop fruits and vegetables.',\n",
       " 'chatgpt_response': 'Introducing the ultimate kitchen tool for busy home cooks - the food chopper! This versatile gadget allows you to easily and efficiently chop a variety of fruits and vegetables, saving you time and energy in the kitchen.\\nThe food chopper features a sharp, stainless steel blade that effortlessly slices through even the toughest produce, including carrots, onions, celery, and more. It also has a compact size that fits easily into most kitchen drawers, making it a convenient choice for even the smallest of kitchens.\\nOne of the standout features of this food chopper is its adjustable slicing blade, which allows you to customize the thickness of your cuts. Whether you want thin, precise slices or thick, chunky pieces, the food chopper has you covered.\\nThe food chopper also comes with a handy safety guard, which prevents the blade from accidentally coming into contact with your fingers as you chop. This added layer of protection makes it a safe choice for both experienced and beginner cooks alike.\\nCleaning the food chopper is a breeze, as it is dishwasher safe and easy to disassemble.\\nSimply remove the blade and place it in the dishwasher, or hand wash it in warm, soapy water.\\nIn summary, the food chopper is a must-have kitchen tool that makes chopping fruits and vegetables a breeze. Its sharp blade, adjustable slicing feature, and safety guard make it a safe and efficient choice for any home cook. So why wait? Add this handy gadget to your kitchen today and make meal prep a breeze!'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0691cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset2['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145fb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f1 = open('datasets/Hello-SimpleAI-HC3-all.jsonl')\n",
    "f2 = open('datasets/MohamedRashad-ChatGPT-prompts.jsonl')\n",
    "Lines1 = f1.readlines()\n",
    "Lines2 = f2.readlines()\n",
    "\n",
    "questions, answers = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4b7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset has columns for question, human_answers, and chatgpt_answers\n",
    "for line in Lines1:\n",
    "    row = json.loads(line)\n",
    "    # pair up the questions and answers so that the human question goes in the questions list as \"Human: \"\n",
    "    # and then the HUMAN answer goes in the answers list as \"Assistant: \"\n",
    "    # we need to iterate through this in a for loop since there could be multiple answers in the list\n",
    "    for answer in row[\"human_answers\"]:\n",
    "        questions.append(\"Human: \"+row[\"question\"])\n",
    "        answers.append(\"Assistant: \"+answer)\n",
    "    # pair up the questions and answers so that the human question goes in the questions list as \"Human: \"\n",
    "    # and then the CHATGPT answer goes in the answers list as \"Assistant: \"\n",
    "    # we need to iterate through this in a for loop since there could be multiple answers in a list\n",
    "    for answer in row[\"chatgpt_answers\"]:\n",
    "        questions.append(\"Human: \"+row[\"question\"])\n",
    "        answers.append(\"Assistant: \"+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b94d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in Lines2:\n",
    "    row = json.loads(line)\n",
    "    questions.append(\"Human: \" + row['human_prompt'])\n",
    "    answers.append(\"Assistant: \" + row['chatgpt_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b0de14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85809"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f20bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"question\"] = questions\n",
    "df[\"answer\"] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d7d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataframe\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "844b182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the split index\n",
    "split_idx = int(0.8 * len(df))\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df = df[:split_idx]\n",
    "val_df = df[split_idx:]\n",
    "\n",
    "# Shuffle the datasets\n",
    "train_df = train_df.sample(frac=1, random_state=42)\n",
    "val_df = val_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Save the dataframes\n",
    "train_df.to_csv('datasets/train.csv', index=False)\n",
    "val_df.to_csv('datasets/val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddf1de",
   "metadata": {},
   "source": [
    "# Training in Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14121c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup, DataCollatorForSeq2Seq\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, get_peft_model_state_dict\n",
    "from peft.utils.other import fsdp_auto_wrap_policy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0f1ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72b444b39e84a769dd5bf541cdfe50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939c600779df4bc4aed68ce24f72174c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e371f18a4444f04b5be6705edab26c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884736 || all params: 248462592 || trainable%: 0.3560841867092814\n",
      "None\n",
      "Downloading and preparing dataset csv/default to /workspace/notebooks/cache/csv/default-c841913b1c311d76/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b2bbcbdf784968ab8cd3492370372e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2f752ac8a94bc4b96b68d9e08a1583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /workspace/notebooks/cache/csv/default-c841913b1c311d76/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb37f6488274758a25399a433e993fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad474aa328f5455687af86375cbfaf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624af563b4e7493897e8d0840581c6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dc34c1fa704ddead21784194162336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5af75ea82b749fb9458f08f8653b78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "# google/flan-t5-xxl\n",
    "# google/flan-t5-small\n",
    "model_name_or_path = \"google/flan-t5-base\"\n",
    "batch_size = 2\n",
    "max_length = 512\n",
    "lr = 1e-4\n",
    "num_epochs = 1\n",
    "train_data = \"./datasets/train.csv\"\n",
    "test_data = \"./datasets/val.csv\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "checkpoint_name = \"chaT5_lora.pt\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "accelerator.print(model.print_trainable_parameters())\n",
    "\n",
    "dataset = load_dataset(\n",
    "        'csv', data_files={\n",
    "            \"train\": train_data,\n",
    "            \"validation\": test_data,\n",
    "        },\n",
    "        cache_dir=\"./cache\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c611f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"question\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_length, padding=True, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"answer\"], max_length=max_length, padding=True, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49107d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=16):   0%|          | 0/68647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=16):   0%|          | 0/17162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3614: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with accelerator.main_process_first():\n",
    "    processed_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=16,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec49938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer, model=model)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bace4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): T5ForConditionalGeneration(\n",
      "      (shared): Embedding(32128, 768)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear(\n",
      "                    in_features=768, out_features=768, bias=False\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34324 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 2/34324 [00:01<4:30:33,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(39.1156, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1001/34324 [01:41<1:04:19,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.6671, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2002/34324 [03:22<53:44, 10.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.4336, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 3001/34324 [05:03<54:14,  9.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.0071, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 4003/34324 [06:45<48:44, 10.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.4910, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 5003/34324 [08:27<47:06, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.5078, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6001/34324 [10:08<51:36,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.7891, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 7002/34324 [11:49<45:00, 10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.6873, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 8001/34324 [13:31<48:02,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.9666, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 9002/34324 [15:13<41:31, 10.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.1884, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 10001/34324 [16:54<46:21,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.1447, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 11002/34324 [18:36<39:25,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.0574, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 12001/34324 [20:18<39:26,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.3868, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 13001/34324 [21:59<43:14,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.9574, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 14002/34324 [23:41<33:06, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.8006, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 15001/34324 [25:23<33:08,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.9958, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 16003/34324 [27:06<30:02, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.5529, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 17003/34324 [28:48<28:21, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.7988, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 18003/34324 [30:29<27:01, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(2.0399, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 19001/34324 [32:10<29:38,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.7588, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 20001/34324 [33:51<26:56,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.4634, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 21002/34324 [35:33<22:50,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.8644, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 22002/34324 [37:16<22:39,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.0744, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 23002/34324 [38:58<18:23, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.7928, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 24003/34324 [40:40<16:48, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.0985, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 25002/34324 [42:22<15:45,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.7740, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 26003/34324 [44:04<13:31, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.3508, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 27001/34324 [45:45<12:33,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.5581, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 28002/34324 [47:27<10:53,  9.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.4060, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 29003/34324 [49:09<08:51, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.8106, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 30003/34324 [50:50<07:15,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.3520, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 31003/34324 [52:32<05:38,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.7487, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 32001/34324 [54:13<04:08,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.3372, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 33001/34324 [55:55<02:17,  9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.9115, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 34002/34324 [57:37<00:36,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.0359, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34324/34324 [58:10<00:00,  9.83it/s]\n",
      "100%|██████████| 8581/8581 [05:57<00:00, 24.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(91.7621, device='cuda:0') train_epoch_loss=tensor(4.5192, device='cuda:0') eval_ppl=tensor(1.2462, device='cuda:0') eval_epoch_loss=tensor(0.2201, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\n",
    "    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n",
    "\n",
    "model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\n",
    "    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\n",
    ")\n",
    "accelerator.print(model)\n",
    "#accelerator.state.deepspeed_plugin.zero_stage == 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step%1000 == 0:\n",
    "            print(\"loss: \",loss.detach().float())\n",
    "            accelerator.wait_for_everyone()\n",
    "            if accelerator.is_main_process:\n",
    "                accelerator.save(\n",
    "                    get_peft_model_state_dict(model, state_dict=accelerator.get_state_dict(model)), checkpoint_name\n",
    "                )\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        preds = accelerator.gather_for_metrics(torch.argmax(outputs.logits, -1)).detach().cpu().numpy()\n",
    "        eval_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n",
    "    eval_epoch_loss = eval_loss / len(train_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(eval_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    accelerator.print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    accelerator.save(\n",
    "        get_peft_model_state_dict(model, state_dict=accelerator.get_state_dict(model)), checkpoint_name\n",
    "    )\n",
    "    accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33119ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how I figured this out:\n",
    "# https://huggingface.co/blog/peft\n",
    "model.save_pretrained(\"flan-t5-base-lora\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ef18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then put it in the loras folder of the oobabooga webui and then run this:\n",
    "# python server.py --model google_flan-t5-small --lora luke_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
